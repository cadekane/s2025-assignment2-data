{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 HTML to text conversion\n",
    "\n",
    "NOTE: it is gzcat on MacOS to view the files…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resiliparse.parse.encoding import detect_encoding\n",
    "from resiliparse.extract.html2text import extract_plain_text\n",
    "\n",
    "def run_extract_text_from_html_bytes(html_bytes: bytes) -> str | None:\n",
    "\n",
    "    # Detect encoding of the byte string\n",
    "    enc = detect_encoding(html_bytes)\n",
    "    print(enc)\n",
    "\n",
    "    # Decode the byte string into a Unicode string\n",
    "    html = html_bytes.decode('utf-8')\n",
    "    print(html)\n",
    "\n",
    "    # If the encoding is not UTF-8, try to decode it using the detected encoding\n",
    "    if enc != 'utf-8':\n",
    "        try:\n",
    "            html = html_bytes.decode(enc)\n",
    "        except UnicodeDecodeError:\n",
    "            return None\n",
    "\n",
    "    # Extract text from the HTML string\n",
    "    text = extract_plain_text(html)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp1252\n",
      "<html><head><title>Test</title></head><body><h1>Hello World</h1></body></html>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello World'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run_extract_text_from_html_bytes(b'\\xff\\xfeH\\x00e\\x00l\\x00l\\x00o\\x00 \\x00W\\x00o\\x00r\\x00l\\x00d\\x00')\n",
    "run_extract_text_from_html_bytes(b'<html><head><title>Test</title></head><body><h1>Hello World</h1></body></html>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/CC-MAIN-20210722174000-20210722194000-00111.warc.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m text:\n\u001b[1;32m     15\u001b[0m                     \u001b[38;5;28mprint\u001b[39m(text[:\u001b[38;5;241m500\u001b[39m])  \u001b[38;5;66;03m# Print first 500 characters as a preview\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mprocess_warc_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/CC-MAIN-20210722174000-20210722194000-00111.warc.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mprocess_warc_file\u001b[0;34m(warc_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_warc_file\u001b[39m(warc_path):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mgzip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwarc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m warc_gz:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m ArchiveIterator(warc_gz):\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m record\u001b[38;5;241m.\u001b[39mrecord_type \u001b[38;5;241m==\u001b[39m WarcRecordType\u001b[38;5;241m.\u001b[39mresponse:\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_data/lib/python3.10/gzip.py:58\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m gz_mode \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)):\n\u001b[0;32m---> 58\u001b[0m     binary_file \u001b[38;5;241m=\u001b[39m \u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgz_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     60\u001b[0m     binary_file \u001b[38;5;241m=\u001b[39m GzipFile(\u001b[38;5;28;01mNone\u001b[39;00m, gz_mode, compresslevel, filename)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_data/lib/python3.10/gzip.py:174\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    172\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/CC-MAIN-20210722174000-20210722194000-00111.warc.gz'"
     ]
    }
   ],
   "source": [
    "from fastwarc import ArchiveIterator, WarcRecordType\n",
    "import gzip\n",
    "\n",
    "from resiliparse.parse.encoding import detect_encoding\n",
    "from resiliparse.extract.html2text import extract_plain_text\n",
    "\n",
    "# Function to process a WARC file\n",
    "def process_warc_file(warc_path):\n",
    "    with gzip.open(warc_path, 'rb') as warc_gz:\n",
    "        for record in ArchiveIterator(warc_gz):\n",
    "            if record.record_type == WarcRecordType.response:\n",
    "                html_bytes = record.reader.read()\n",
    "                text = run_extract_text_from_html_bytes(html_bytes)\n",
    "                if text:\n",
    "                    print(text[:500])  # Print first 500 characters as a preview\n",
    "\n",
    "process_warc_file(\"/data/CC-MAIN-20210722174000-20210722194000-00111.warc.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "def run_identify_language(text: str) -> tuple[str, float]:\n",
    "    \n",
    "    model = fasttext.load_model('lid.176.bin')\n",
    "\n",
    "    # Predict the language of the text\n",
    "    text = text.replace('\\n', ' ') # Remove newlines\n",
    "    predictions = model.predict(text, k=1) # k=1 means we only want the top prediction\n",
    "\n",
    "    return (predictions[0][0], predictions[1][0]) # Return the language code and the confidence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask emails function\n",
    "\n",
    "def run_mask_emails(text: str) -> tuple[str, int]:\n",
    "    import re\n",
    "\n",
    "    # Define the regular expression pattern for email addresses\n",
    "    email_pattern = r'[\\w\\.-]+@[\\w\\.-]+'\n",
    "\n",
    "    # Find all email addresses in the text\n",
    "    emails = re.findall(email_pattern, text)\n",
    "\n",
    "    # Replace each email address with a placeholder\n",
    "    masked_text = re.sub(email_pattern, '|||EMAIL_ADDRESS|||', text)\n",
    "\n",
    "    return (masked_text, len(emails)) # returns the masked string and the number of emails found that were masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"I emailed cadekane@hawaii.edu and returned from that guy at chaserp@gmail.com and he sent it to airmed10@yahoo.com\"\n",
    "\n",
    "test_output = run_mask_emails(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I emailed |||EMAIL_ADDRESS||| and returned from that guy at |||EMAIL_ADDRESS||| and he sent it to |||EMAIL_ADDRESS|||',\n",
       " 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to mask out PHONE NUMBERS\n",
    "\n",
    "def run_mask_phone_numbers(text: str) -> tuple[str, int]:\n",
    "    import re\n",
    "\n",
    "    # Define the regular expression pattern for phone numbers\n",
    "    # phone_pattern = r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b'\n",
    "    phone_pattern = r'(\\+\\d{1,2}\\s)?\\(?\\d{3}\\)?[\\s.-]\\d{3}[\\s.-]\\d{4}'\n",
    "    phone_pattern = r'(\\+\\d{1,2}\\s)?\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}'\n",
    "\n",
    "    # Find all phone numbers in the text\n",
    "    phones = re.findall(phone_pattern, text)\n",
    "\n",
    "    # Replace each phone number with a placeholder\n",
    "    masked_text = re.sub(phone_pattern, '|||PHONE_NUMBER|||', text)\n",
    "\n",
    "    return (masked_text, len(phones)) # returns the masked string and the number of phone numbers found that were masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phone = \"\"\"Hello there here are the 2831823829, 123-456-7890, (123) 456-7890, 123 456 7890, 123.456.7890, +91 (123) 456-7890 phone numbers\"\"\"\n",
    "\n",
    "test_output = run_mask_phone_numbers(test_phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Hello there here are the |||PHONE_NUMBER|||, |||PHONE_NUMBER|||, |||PHONE_NUMBER|||, |||PHONE_NUMBER|||, |||PHONE_NUMBER|||, |||PHONE_NUMBER||| phone numbers',\n",
       " 6)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to mask out IP addresses\n",
    "# Only have to focus on IPv4 addresses\n",
    "\n",
    "def run_mask_ips(text: str) -> tuple[str, int]:\n",
    "    import re\n",
    "\n",
    "    # Define the regular expression pattern for IP addresses\n",
    "    ip_pattern = r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b' # for IPv4 addresses\n",
    "\n",
    "    ips = re.findall(ip_pattern, text)\n",
    "\n",
    "    masked_test = re.sub(ip_pattern, '|||IP_ADDRESS|||', text)\n",
    "\n",
    "    return (masked_test, len(ips))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Harmful content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSFW classifier\n",
    "\n",
    "def run_classify_nsfw(text: str) -> tuple[Any, float]:\n",
    "    \n",
    "    model = fasttext.load_model('jigsaw_fasttext_bigrams_nsfw_final.bin')\n",
    "\n",
    "    # Predict the probability of the text being NSFW\n",
    "    text = text.replace('\\n', ' ') # Remove newlines\n",
    "\n",
    "    predictions = model.predict(text, k=1) # k=1 means we only want the top prediction\n",
    "\n",
    "    predicted_nsfw = predictions[0][0].replace('__label__', '') # Remove the '__label__' prefix\n",
    "    confidence_score = predictions[1][0]\n",
    "\n",
    "    return (predicted_nsfw, confidence_score) # Return the language code and the confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toxic speech classifier\n",
    "\n",
    "def run_classify_toxic_speech(text: str) -> tuple[Any, float]:\n",
    "    \n",
    "    model = fasttext.load_model('jigsaw_fasttext_bigrams_hatespeech_final.bin')\n",
    "\n",
    "    # Predict the probability of the text being NSFW\n",
    "    text = text.replace('\\n', ' ') # Remove newlines\n",
    "\n",
    "    predictions = model.predict(text, k=1) # k=1 means we only want the top prediction\n",
    "\n",
    "    predicted_speech = predictions[0][0].replace('__label__', '') # Remove the '__label__' prefix\n",
    "    confidence_score = predictions[1][0]\n",
    "\n",
    "    return (predicted_speech, confidence_score) # Return the language code and the confidence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Quality filter (golpher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cadekane/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Filter out documents that\n",
    "# • Contain less than 50 or more than 100,000 words. \n",
    "# • Have a mean word length outside the range of 3 to 10 characters. \n",
    "# • Have more than 30% of lines ending with an ellipsis (”...”). \n",
    "# • Contain less than 80% of words with at least one alphabetic character.\n",
    "\n",
    "# use the NLTK package nltk.word_tokenize for tokenizing text into words. Then, you can count them, calculate the mean word length, etc.\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')  # Ensure tokenizer is available\n",
    "\n",
    "def run_gopher_quality_filter(text: str) -> bool:\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Filter out documents that contain less than 50 or more than 100,000 words\n",
    "    if len(words) < 50 or len(words) > 100000:\n",
    "        return False\n",
    "\n",
    "    # Calculate the mean word length and filter out if it is outside the range of 3 to 10 characters\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words) if len(words) > 0 else 0\n",
    "    if avg_word_length < 3 or avg_word_length > 10:\n",
    "        return False\n",
    "\n",
    "    # Filter out documents that have more than 30% of lines ending with an ellipsis\n",
    "    lines = text.split('\\n')\n",
    "    ellipsis_count = sum(line.endswith('...') for line in lines)\n",
    "    ellipsis_ratio = ellipsis_count / len(lines) if len(lines) > 0 else 0\n",
    "    if ellipsis_ratio > 0.3:\n",
    "        return False\n",
    "\n",
    "    # Filter out documents that contain less than 80% of words with at least one alphabetic character\n",
    "    alpha_word_count = sum(any(c.isalpha() for c in word) for word in words)\n",
    "    alpha_ratio = alpha_word_count / len(words) if len(words) > 0 else 0\n",
    "    if alpha_ratio < 0.8:\n",
    "        return False\n",
    "\n",
    "    return True  # Passes all filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '!']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"Hello, world!\", preserve_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/cadekane/nltk_data'\n    - '/Users/cadekane/miniconda3/envs/cs336_data/nltk_data'\n    - '/Users/cadekane/miniconda3/envs/cs336_data/share/nltk_data'\n    - '/Users/cadekane/miniconda3/envs/cs336_data/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m example_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mThis is a test document with 100 words. It has a mean word length of 4.5 characters.\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mIt has 3 lines, 1 of which ends with an ellipsis. It contains 90\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[38;5;124mf words with at least one alphabetic character...\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mrun_gopher_quality_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 15\u001b[0m, in \u001b[0;36mrun_gopher_quality_filter\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_gopher_quality_filter\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Tokenize the text into words\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Filter out documents that contain less than 50 or more than 100,000 words\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(words) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(words) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m100000\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_data/lib/python3.10/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_data/lib/python3.10/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_data/lib/python3.10/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_data/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_data/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/miniconda3/envs/cs336_data/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/cadekane/nltk_data'\n    - '/Users/cadekane/miniconda3/envs/cs336_data/nltk_data'\n    - '/Users/cadekane/miniconda3/envs/cs336_data/share/nltk_data'\n    - '/Users/cadekane/miniconda3/envs/cs336_data/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "example_text = \"\"\"This is a test document with 100 words. It has a mean word length of 4.5 characters.\n",
    "It has 3 lines, 1 of which ends with an ellipsis. It contains 90% of words with at least one alphabetic character...\"\"\"\n",
    "\n",
    "run_gopher_quality_filter(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # punkt should be punkt_… something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Exact line deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes a list of paths to input files and performs exact line deduplication on them.\n",
    "\n",
    "def run_exact_line_deduplication(\n",
    "    input_files: list[os.PathLike], output_directory: os.PathLike\n",
    "):\n",
    "    # Ensure the output directory exists and make it if it doesn't\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # First pass: Count frequency of each line, using the hash of the line as the key\n",
    "    line_counts = {}\n",
    "    for path in input_paths:\n",
    "        with open(path, 'r') as file:\n",
    "            for line in file:\n",
    "                # Compute the hash of the line\n",
    "                line_hash = hash(line) # hash() function computes a hash value of the input\n",
    "                line_counts[line_hash] = line_counts.get(line_hash, 0) + 1\n",
    "    \n",
    "    # Second pass: write each file's unique lines (those with FREQUENCY 1) to the output directory\n",
    "    for path in input_paths:\n",
    "        # Preserve the file name in the output directory\n",
    "        output_path = os.path.join(output_directory, os.path.basename(path))\n",
    "        with open(path, 'r') as file, open(output_path, 'w') as output_file:\n",
    "            for line in file:\n",
    "                line_hash = hash(line)\n",
    "                # Only write the line if it occurs exactly once in the whole corpus.\n",
    "                if line_counts[line_hash] == 1:\n",
    "                    output_file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 MinHash + LSH document deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_minhash_deduplication(\n",
    "    input_files: list[os.PathLike],\n",
    "    num_hashes: int,\n",
    "    num_bands: int,\n",
    "    ngrams: int,\n",
    "    jaccard_threshold: float,\n",
    "    output_directory: os.PathLike,\n",
    "):\n",
    "    # 1. Compute minhash signatures for each document\n",
    "    minhash_signatures = {}\n",
    "\n",
    "    for path in input_files:\n",
    "        with open(path, 'r') as file:\n",
    "            text = file.read()\n",
    "            minhash_signatures[path] = compute_minhash_signature(text, num_hashes, ngrams) # finished but unsure if works\n",
    "\n",
    "    # 2. Use LSH with the provided # of bands to identify candidate duplicates\n",
    "    candidate_pairs = lsh(minhash_signatures, num_bands) # unfinished\n",
    "\n",
    "    # 3. Compute Jaccard similarity for candidate pairs and cluster pairs with common documents, such as pair AB and BC into ABC\n",
    "    clusters = cluster_candidates(candidate_pairs, minhash_signatures, jaccard_threshold) # unfinished\n",
    "\n",
    "    # 4. Remove a random document from each duplicate cluster and write the remaining documents to the output directory\n",
    "    for cluster in clusters:\n",
    "        # Choose a random document to discard\n",
    "        discard_doc = random.choice(cluster)\n",
    "        # Write the remaining documents to the output directory\n",
    "        for doc in cluster:\n",
    "            if doc != discard_doc:\n",
    "                output_path = os.path.join(output_directory, os.path.basename(doc))\n",
    "                shutil.copy(doc, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def compute_minhash_signature(text: str, num_hashes: int, ngrams: int) -> list[int]:\n",
    "    # Tokenize the text into n-grams\n",
    "    n_grams = set(nltk.ngrams(text, n=ngrams)) # n number of n-grams? \n",
    "\n",
    "    # Initialize the signature with infinity for each hash function\n",
    "    signature = [float('inf')] * num_hashes\n",
    "\n",
    "    # Generate hash functions\n",
    "    hash_funcs = generate_hash_functions(num_hashes) # k hash functions\n",
    "\n",
    "    # Is this correct? I'm not sure if hash func for loop should be outside the n_gram for loop\n",
    "    # Update the signature for each n-gram\n",
    "    for n_gram in n_grams:\n",
    "        for i, hash_func in enumerate(hash_funcs):\n",
    "            hash_val = hash_func(n_gram)\n",
    "            signature[i] = min(signature[i], hash_val)\n",
    "    \n",
    "    return signature # size of num_hashes\n",
    "\n",
    "def generate_hash_functions(num_hashes: int):\n",
    "    # Generate a list of hash functions using random seeds\n",
    "    hash_funcs = []\n",
    "    for seed in range(num_hashes):\n",
    "        hash_funcs.append(generate_hash_function(seed))\n",
    "    return hash_funcs\n",
    "\n",
    "def generate_hash_function(seed: int):\n",
    "    # Generate a hash function using the given seed\n",
    "    def hash_func(n_gram):\n",
    "        return hash(n_gram) ^ seed\n",
    "    return hash_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def lsh(minhash_signatures: dict, num_bands: int) -> set:\n",
    "    \"\"\"\n",
    "    Given a dictionary mapping document IDs (or file names) to their MinHash signatures,\n",
    "    this function splits each signature into `num_bands` and buckets documents that share the\n",
    "    same band signature. If two documents share a band, they are added as a candidate pair.\n",
    "\n",
    "    Args:\n",
    "        minhash_signatures (dict): A mapping from document ID to its MinHash signature (list of ints).\n",
    "        num_bands (int): The number of bands to split each signature into.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of tuples, each tuple containing a pair of document IDs that are candidate duplicates.\n",
    "    \"\"\"\n",
    "    buckets = defaultdict(list)\n",
    "    candidate_pairs = set()\n",
    "    \n",
    "    # Assuming all signatures are of equal length.\n",
    "    sample_signature = next(iter(minhash_signatures.values()))\n",
    "    sig_length = len(sample_signature)\n",
    "    band_size = sig_length // num_bands\n",
    "    # Note: This assumes sig_length is exactly divisible by num_bands.\n",
    "    \n",
    "    # Process each document and its signature.\n",
    "    for doc_id, signature in minhash_signatures.items():\n",
    "        for band in range(num_bands):\n",
    "            start = band * band_size\n",
    "            end = start + band_size\n",
    "            # Create a tuple that represents the signature for this band.\n",
    "            band_signature = tuple(signature[start:end])\n",
    "            # Use a combination of band index and band_signature as the bucket key.\n",
    "            buckets[(band, band_signature)].append(doc_id) # doc_id is really just the path\n",
    "    \n",
    "    # For each bucket, any documents sharing the same band are candidate duplicates.\n",
    "    for docs in buckets.values():\n",
    "        if len(docs) > 1:\n",
    "            # Generate all unique pairs from the documents in this bucket.\n",
    "            for pair in combinations(sorted(docs), 2): # combinations returns all unique pairs\n",
    "                candidate_pairs.add(pair)\n",
    "    \n",
    "    return candidate_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "def jaccard_similarity(set1: set, set2: set) -> float: # how exactly to compute jaccard similarity between two documents?\n",
    "    \"\"\"Computes the Jaccard similarity between two sets.\"\"\"\n",
    "    intersection = set1 & set2\n",
    "    union = set1 | set2\n",
    "    return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "class UnionFind:\n",
    "    def __init__(self):\n",
    "        self.parent = {}\n",
    "\n",
    "    def find(self, a):\n",
    "        # Initialize the parent of 'a' to itself if not present.\n",
    "        if a not in self.parent:\n",
    "            self.parent[a] = a\n",
    "        # Path compression.\n",
    "        if self.parent[a] != a:\n",
    "            self.parent[a] = self.find(self.parent[a])\n",
    "        return self.parent[a]\n",
    "\n",
    "    def union(self, a, b):\n",
    "        rootA = self.find(a)\n",
    "        rootB = self.find(b)\n",
    "        if rootA != rootB:\n",
    "            self.parent[rootB] = rootA\n",
    "\n",
    "def cluster_duplicates(candidate_pairs: set, doc_shingles: dict, threshold: float) -> list:\n",
    "    \"\"\"\n",
    "    Clusters duplicate documents based on candidate pairs that exceed the Jaccard similarity threshold.\n",
    "    \n",
    "    Args:\n",
    "        candidate_pairs (set of tuple): Set of candidate duplicate pairs, each represented as a tuple of document IDs.\n",
    "        doc_shingles (dict): A mapping from document ID to its set of tokens (or shingles).\n",
    "        threshold (float): Jaccard similarity threshold above which two documents are considered duplicates.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of clusters, where each cluster is a set of document IDs.\n",
    "    \"\"\"\n",
    "    uf = UnionFind()\n",
    "    \n",
    "    # Process each candidate pair and compute Jaccard similarity.\n",
    "    for doc_a, doc_b in candidate_pairs:\n",
    "        sim = jaccard_similarity(doc_shingles[doc_a], doc_shingles[doc_b])\n",
    "        if sim >= threshold:\n",
    "            uf.union(doc_a, doc_b)\n",
    "    \n",
    "    # Group documents by their root representative to form clusters.\n",
    "    clusters = defaultdict(set)\n",
    "    for doc in doc_shingles:\n",
    "        root = uf.find(doc)\n",
    "        clusters[root].add(doc)\n",
    "    \n",
    "    # Return clusters as a list of sets.\n",
    "    return list(clusters.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "\n",
    "# Assume we have three documents with token sets (shingles)\n",
    "doc_shingles = {\n",
    "    \"doc1\": {\"the\", \"cat\", \"sat\"},\n",
    "    \"doc2\": {\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"},\n",
    "    \"doc3\": {\"dog\", \"barked\"},\n",
    "    \"doc4\": {\"the\", \"cat\", \"sat\"},\n",
    "}\n",
    "\n",
    "# Suppose our LSH step produced the following candidate pairs:\n",
    "candidate_pairs = {(\"doc1\", \"doc2\"), (\"doc1\", \"doc4\"), (\"doc2\", \"doc3\")}\n",
    "\n",
    "# Set a Jaccard similarity threshold.\n",
    "threshold = 0.5\n",
    "\n",
    "clusters = cluster_duplicates(candidate_pairs, doc_shingles, threshold)\n",
    "print(\"Clusters:\", clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
